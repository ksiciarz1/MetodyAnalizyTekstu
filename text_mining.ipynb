{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d23a29",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6bda5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import morfeusz2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator as op\n",
    "import itertools as it\n",
    "import os\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccefe94e",
   "metadata": {},
   "source": [
    "### Korpus dokumentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db68e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = \"./Literatura - original\"\n",
    "corpus = PlaintextCorpusReader(corpus_dir, \".*\\.txt\")\n",
    "files_names = corpus.fileids()\n",
    "files_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b99b6",
   "metadata": {},
   "source": [
    "### Wstępne przygotowanie dokumentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fcff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {}\n",
    "for file in files_names:\n",
    "    documents[file] = corpus.raw(file)\n",
    "print(json.dumps(documents, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc67a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist_file = open(\"./stopwords_pl.txt\", \"r\", encoding=\"UTF-8\")\n",
    "stoplist = stoplist_file.read().splitlines()\n",
    "stoplist_file.close()\n",
    "stoplist = stoplist[4:]\n",
    "stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08311a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    morf = morfeusz2.Morfeusz()\n",
    "    segments = it.groupby(morf.analyse(text), op.itemgetter(0,1))\n",
    "    def disambiguate(group):\n",
    "        pairs = ((len(descr), lemma) for _, _, (_, lemma, descr, _, _) in group)\n",
    "        perpl, lemma = min(pairs)\n",
    "        return lemma.split(':')[0]\n",
    "    lemmas = (disambiguate(group) for key, group in segments)\n",
    "    return \" \".join(filter(str.isalpha, lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f14414",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in documents:\n",
    "    documents[key] = documents[key].lower()\n",
    "    documents[key] = \"\".join([char for char in documents[key] if char not in string.punctuation])\n",
    "    documents[key] = lemmatize(documents[key])\n",
    "    documents[key] = \" \".join([word for word in word_tokenize(documents[key]) if word not in stoplist])\n",
    "print(json.dumps(documents, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaf9e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# morf = morfeusz2.Morfeusz()\n",
    "# morf.analyse(\"Ala ma kota\")\n",
    "lemmatize(\"Ala ma kota\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e595f7a0",
   "metadata": {},
   "source": [
    "### Utworzenie macierzy częstości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd309d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.DataFrame.from_dict(documents, orient=\"index\")\n",
    "docs.columns = ['content']\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d81c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "counts_tfidf = count_vectorizer.fit_transform(docs['content'])\n",
    "counts_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea59069e",
   "metadata": {},
   "source": [
    "### Katalogi na wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde5a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./wordclouds\"):\n",
    "    os.mkdir(\"./wordclouds\")\n",
    "if not os.path.exists(\"./topics\"):\n",
    "    os.mkdir(\"./topics\")\n",
    "if not os.path.exists(\"./clusters\"):\n",
    "    os.mkdir(\"./clusters\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ccc5d4",
   "metadata": {},
   "source": [
    "### Chmury Tagów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f549d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordclound = WordCloud(\n",
    "    background_color=\"white\",\n",
    "    max_words=5000,\n",
    "    contour_width=3,\n",
    "    contour_color=\"steelblue\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c5293",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in docs.iterrows():\n",
    "    wordclound.generate(row['content'])\n",
    "    plt.imshow(wordclound)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(index.replace(\".txt\", \"\"))\n",
    "    plt.savefig(\"./wordclouds/{}\".format(index.replace(\".txt\", \".png\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
